{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b48cf5-bc1f-4aec-bf79-656da41a4bab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss\n",
    "import numpy as np\n",
    "\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self, inplace=False):\n",
    "        super(TestNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=inplace)\n",
    "        self.lin1 = nn.Linear(1, 64)\n",
    "        self.lin2 = nn.Linear(64, 64)\n",
    "        self.lin3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (self.lin1(x))\n",
    "        x = (self.lin2(x))\n",
    "        x = (self.lin3(x))\n",
    "            \n",
    "        return x\n",
    "\n",
    "    \n",
    "const_bnn_prior_parameters = {\n",
    "        \"prior_mu\": 0.0,\n",
    "        \"prior_sigma\": 1.0,\n",
    "        \"posterior_mu_init\": 0.0,\n",
    "        \"posterior_rho_init\": -3.0,\n",
    "        \"type\": \"Reparameterization\",  # Flipout or Reparameterization\n",
    "        \"moped_enable\": False,  # True to initialize mu/sigma from the pretrained dnn weights\n",
    "        \"moped_delta\": 0.5,\n",
    "}\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1545f349-f8d8-44b8-9be1-1f0c07abcc9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_model = TestNet()\n",
    "\n",
    "for i in range(100):\n",
    "    print('model:', torch_model(torch.tensor([[1.37592]])).item())\n",
    "\n",
    "dnn_to_bnn(torch_model, const_bnn_prior_parameters)\n",
    "\n",
    "model_parameters = filter(lambda p: p.requires_grad, torch_model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "ema = ExponentialMovingAverage(torch_model.parameters(), decay=0.99)\n",
    "\n",
    "\n",
    "ema.update()\n",
    "\n",
    "print(ema)\n",
    "torch_model.eval()\n",
    "\n",
    "for i in range(100):\n",
    "    print('bnn:', torch_model(torch.tensor([[1.37592]])).item())\n",
    "\n",
    "with ema.average_parameters():\n",
    "    torch_model.eval()\n",
    "\n",
    "    for i in range(100):\n",
    "        print('ema:', torch_model(torch.tensor([[1.37592]])).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567862f5-35a8-48da-9b70-70d906b88d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64d0294-e2d3-49a1-8c81-bb3d08b33913",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1])\n",
      "bnn: tensor([[-0.1878,  1.1372,  0.9183,  0.1293, -0.7445,  0.6254]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.4981,  0.4849,  1.3352,  0.3834, -0.5673,  1.1622]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.3439,  1.0017,  1.3601,  0.0040, -0.4730,  0.9166]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.6307,  0.9143,  1.2051,  0.2269, -0.5356,  1.1619]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.3558,  0.6975,  1.3203,  0.2691, -0.5945,  0.7414]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.1480,  1.0766,  0.9194, -0.0344, -0.4026,  0.6309]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.4813,  1.1495,  1.2310,  0.1617, -0.4235,  1.0756]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.2635,  1.2918,  1.1798,  0.3109, -0.3731,  1.0160]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.3534,  1.2002,  1.1501,  0.2867, -0.8055,  0.9288]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.3401,  1.1492,  1.2515,  0.0877, -0.3937,  1.0767]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.5567,  0.7257,  1.1916,  0.0670, -0.7102,  0.4309]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.2430,  1.3006,  1.2791,  0.0643, -0.7878,  0.8250]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.4439,  0.8488,  1.3545,  0.1046, -0.6244,  1.3634]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.5472,  0.6862,  1.1305, -0.1088, -0.7817,  0.5753]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.1916,  1.0597,  1.1343,  0.0067, -0.3797,  0.3126]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.3807,  0.7440,  1.3576,  0.2535, -0.9229,  0.8222]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.3606,  0.9874,  1.0330,  0.2566, -0.4945,  0.6723]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.2053,  1.1380,  1.1514,  0.3308, -0.7845,  0.9221]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.3076,  1.0765,  1.4842,  0.3419, -0.8004,  0.6775]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "bnn: tensor([[-0.4607,  1.1849,  0.7915,  0.1873, -0.6940,  0.7375]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.4696,  0.8038,  1.3359,  0.1744, -0.5219,  0.7819]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.2357,  1.1411,  1.2805,  0.1422, -0.4697,  0.6931]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.3569,  1.1447,  1.2996,  0.2000, -0.7612,  0.7218]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.3353,  0.8262,  1.2296,  0.1207, -0.4707,  0.8389]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.4068,  1.2372,  1.2252,  0.1069, -0.9087,  0.8214]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.4225,  0.9980,  1.3546,  0.1061, -0.6529,  0.8049]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.3608,  0.9355,  1.3272,  0.1219, -0.6007,  0.6238]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.5090,  1.0763,  1.1899,  0.0449, -0.5221,  0.9889]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.2754,  1.0141,  1.2424,  0.0230, -0.7907,  0.7838]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.4408,  0.7680,  1.2170,  0.2139, -0.6553,  0.9198]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.2927,  0.9908,  1.3793,  0.0658, -0.5867,  0.8509]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.2486,  1.0140,  1.2993,  0.2656, -0.5244,  0.5643]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.2402,  0.7187,  1.1361, -0.0186, -0.7047,  0.4221]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.1739,  1.1999,  1.2559,  0.0317, -0.5957,  0.7819]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.2507,  0.8690,  1.0834,  0.0722, -0.5950,  0.8205]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.4831,  1.0181,  1.1763,  0.1663, -0.8016,  0.9287]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.3066,  1.1231,  1.2072,  0.0200, -0.8638,  0.6450]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.4201,  0.9403,  1.1566,  0.2489, -0.6239,  0.8576]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.3586,  0.7658,  1.3763,  0.1669, -0.5286,  0.8434]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "ema: tensor([[-0.2150,  1.0807,  1.0827,  0.0590, -0.6090,  0.9261]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Copyright (c) 2023, NVIDIA CORPORATION. All rights reserved.\n",
    "#\n",
    "# This work is licensed under the NVIDIA Source Code License\n",
    "# for I2SB. To view a copy of this license, see the LICENSE file.\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "import torchvision.utils as tu\n",
    "import torchmetrics\n",
    "\n",
    "import distributed_util as dist_util\n",
    "\n",
    "import util\n",
    "from network import  DenseNet\n",
    "from diffusion import Diffusion\n",
    "\n",
    "from ipdb import set_trace as debug\n",
    "\n",
    "\n",
    "from bayesian_torch.models.dnn_to_bnn import dnn_to_bnn, get_kl_loss\n",
    "\n",
    "device ='cpu'\n",
    "\n",
    "\n",
    "class Runner_physics(object):\n",
    "    def __init__(self):\n",
    "        super(Runner_physics,self).__init__()\n",
    "        log = None\n",
    "        \n",
    "        t0=0.001\n",
    "        T=1\n",
    "        interval=1000\n",
    "        \n",
    "        noise_levels = torch.linspace(t0, T, interval, device=device) * interval\n",
    "\n",
    "        self.net = DenseNet(log, noise_levels=noise_levels, use_fp16=False, cond=False, x_dim=6, bayes=True)\n",
    "        \n",
    "\n",
    "        #const_bnn_prior_parameters = {\n",
    "        #        \"prior_mu\": 0.0,\n",
    "        #        \"prior_sigma\": 1.0,\n",
    "        #        \"posterior_mu_init\": 0.0,\n",
    "        #        \"posterior_rho_init\": -3.0,\n",
    "        #        \"type\": \"Reparameterization\",  # Flipout or Reparameterization\n",
    "        #        \"moped_enable\": False,  # True to initialize mu/sigma from the pretrained dnn weights\n",
    "        #        \"moped_delta\": 0.5,\n",
    "        #}\n",
    "    \n",
    "        #dnn_to_bnn(self.net, const_bnn_prior_parameters)\n",
    "        \n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.net.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        self.ema = ExponentialMovingAverage(self.net.parameters(), decay=0.99)\n",
    "   \n",
    "        load = './results/BSBUnfold_OmniFold_train_small/latest.pt'\n",
    "\n",
    "        checkpoint = torch.load(load, map_location=\"cpu\")\n",
    "        #self.net.load_state_dict(checkpoint['net'])\n",
    "        #self.ema.load_state_dict(checkpoint[\"ema\"])\n",
    "\n",
    "        self.net.to(device)\n",
    "        self.ema.to(device)\n",
    "        step = 1\n",
    "        xt = torch.full([1,6], 0.5)\n",
    "        step = torch.full((xt.shape[0],), step, device=device, dtype=torch.long)\n",
    "\n",
    "        ## get KLD:\n",
    "            kld = 0\n",
    "            for name,module in self.net.named_modules():\n",
    "                if 'VBLinear' in str(type(module)):\n",
    "                    module.reset_random()\n",
    "\n",
    "        \n",
    "        print(step)\n",
    "        self.net.eval()\n",
    "        for i in range(20):\n",
    "            for name, module in self.net.named_modules():\n",
    "                if 'VBLinear' in str(type(module)):\n",
    "                    module.reset_random()\n",
    "            \n",
    "            print('bnn:', self.net(xt, step))\n",
    "\n",
    "            \n",
    "            \n",
    "        with self.ema.average_parameters():\n",
    "            self.net.eval()\n",
    "\n",
    "            for i in range(20):\n",
    "                for name, module in self.net.named_modules():\n",
    "                    if 'VBLinear' in str(type(module)):\n",
    "                        module.reset_random()\n",
    "\n",
    "                print('ema:', self.net(xt, step))\n",
    "\n",
    "runner = Runner_physics()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061eb8e8-1823-4632-b2a2-b0d913481c76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "hist_list = []\n",
    "\n",
    "bins = None\n",
    "\n",
    "for i_ens in range(10):\n",
    "    #file = np.load('recon_BSBUnfold_OmniFold_train_small_ens{}.npy'.format(i_ens))\n",
    "    file = np.load('recon_OmniFold_train_small.h5_ens{}.npy'.format(i_ens))\n",
    "    \n",
    "    if bins is None:\n",
    "        hist, bins = np.histogram(file[:,0], bins=50, range=None)\n",
    "        hist_list.append(hist)\n",
    "    else:\n",
    "        hist, _ = np.histogram(file[:,0], bins=bins)\n",
    "        hist_list.append(hist)\n",
    "\n",
    "hist_list = np.array(hist_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe346dfe-0408-437f-b06c-e856125916f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(np.std(hist_list,0)/np.mean(hist_list,0))\n",
    "\n",
    "print(np.mean(hist_list,0))\n",
    "print(np.std(hist_list,0))\n",
    "print(hist_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab171559-1129-413b-96b9-d6f15b74a506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDiefenbacher_pytorch201",
   "language": "python",
   "name": "sdiefenbacher_pytorch201"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
